# SLED
The official repository for Efficient Long-Text Understanding Using Short-Text Models (Ivgi et al., 2022) [paper](https://arxiv.org/abs/2208.00748.pdf).
The data for this paper is hosted on the dataset hub [here](https://huggingface.co/datasets/tau/sled). 
It is based on the [SCROLLS dataset](https://huggingface.co/datasets/tau/scrolls) ([paper](https://arxiv.org/pdf/2201.03533.pdf)), the [SQuAD 1.1 dataset](https://huggingface.co/datasets/squad) ([paper](https://arxiv.org/pdf/1606.05250.pdf)) and the [HotpotQA dataset](https://huggingface.co/datasets/hotpot_qa) ([paper](https://arxiv.org/pdf/1809.09600.pdf)).
It doesn't contain any unpblished data, but includes the configuration needed for the paper.

The code will be uploaded here in the following week.

If you use this repository, please cite as below:
```
@inproceedings{Ivgi2022EfficientLU,
  title={Efficient Long-Text Understanding with Short-Text Models},
  author={Maor Ivgi and Uri Shaham and Jonathan Berant},
  year={2022}
}
```
